{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ad6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d759b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81325322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat(d, catID, maxLength, includeCat = True, includeReview = True, includeLength = True):\n",
    "    feat = []\n",
    "    if includeCat:\n",
    "        # My implementation is modular such that this one function concatenates all three features together,\n",
    "        # depending on which are selected\n",
    "        \n",
    "        # One-hot encoding for beer style\n",
    "        style = d['beer/style']\n",
    "        onehot = [0] * len(catID)\n",
    "        if style in catID:\n",
    "            onehot[catID[style]] = 1\n",
    "        feat += onehot\n",
    "    if includeReview:\n",
    "        review_overall = d['review/aroma']\n",
    "        feat.append(review_overall)\n",
    "    if includeLength:\n",
    "        # Normalized review length\n",
    "        review_length = len(d['review/text'])\n",
    "        normalized_length = review_length / maxLength if maxLength > 0 else 0\n",
    "        feat.append(normalized_length)\n",
    "\n",
    "    return feat + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e864eb33-67b1-45f7-9844-5c4b7d9992ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BER(yTrue, yPred):\n",
    "    # Balanced Error Rate: average of false positive rate and false negative rate\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(yTrue, yPred).ravel()\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "    return 0.5 * (fpr + fnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ada384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(reg, catID, dataTrain, dataValid, dataTest, includeCat=True, includeReview=True, includeLength=True):\n",
    "    mod = linear_model.LogisticRegression(C=reg, class_weight='balanced')\n",
    "\n",
    "    maxLength = max([len(d['review/text']) for d in dataTrain])\n",
    "    \n",
    "    Xtrain = [feat(d, catID, maxLength, includeCat, includeReview, includeLength) for d in dataTrain]\n",
    "    Xvalid = [feat(d, catID, maxLength, includeCat, includeReview, includeLength) for d in dataValid]\n",
    "    Xtest = [feat(d, catID, maxLength, includeCat, includeReview, includeLength) for d in dataTest]\n",
    "    \n",
    "    yTrain = [d['beer/ABV'] > 7 for d in dataTrain]\n",
    "    yValid = [d['beer/ABV'] > 7 for d in dataValid]\n",
    "    yTest = [d['beer/ABV'] > 7 for d in dataTest]\n",
    "    \n",
    "    # (1) Fit the model on the training set\n",
    "    mod.fit(Xtrain, yTrain)\n",
    "\n",
    "    # (2) Compute validation BER\n",
    "    yValidPred = mod.predict(Xvalid)\n",
    "    yTestPred = mod.predict(Xtest)\n",
    "\n",
    "    # (3) Compute test BER\n",
    "    vBER = BER(yValid, yValidPred)\n",
    "    tBER = BER(yTest, yTestPred)\n",
    "    \n",
    "    return mod, vBER, tBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6460933e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08711296",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f872f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q1(catID, dataTrain, dataValid, dataTest):\n",
    "    # No need to modify this if you've implemented the functions above\n",
    "    mod, validBER, testBER = pipeline(10, catID, dataTrain, dataValid, dataTest, True, False, False)\n",
    "    return mod, validBER, testBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84af258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf88be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd1768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q2(catID, dataTrain, dataValid, dataTest):\n",
    "    mod, validBER, testBER = pipeline(10, catID, dataTrain, dataValid, dataTest, True, True, True)\n",
    "    return mod, validBER, testBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7adad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda7330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8985755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q3(catID, dataTrain, dataValid, dataTest):\n",
    "    # Your solution here...\n",
    "    \n",
    "    maxLength = max([len(d['review/text']) for d in dataTrain])\n",
    "    bestBER = float('inf')\n",
    "    bestModel = None\n",
    "    bestValidBER = None\n",
    "    bestTestBER = None\n",
    "    \n",
    "    for c in [0.001, 0.01, 0.1, 1, 10]:\n",
    "        mod = linear_model.LogisticRegression(C=c, class_weight='balanced', max_iter=1000)\n",
    "\n",
    "        Xtrain = [feat(d, catID, maxLength) for d in dataTrain]\n",
    "        Xvalid = [feat(d, catID, maxLength) for d in dataValid]\n",
    "        Xtest = [feat(d, catID, maxLength) for d in dataTest]\n",
    "\n",
    "        yTrain = [d['beer/ABV'] > 7 for d in dataTrain]\n",
    "        yValid = [d['beer/ABV'] > 7 for d in dataValid]\n",
    "        yTest = [d['beer/ABV'] > 7 for d in dataTest]\n",
    "\n",
    "        mod.fit(Xtrain, yTrain)\n",
    "        \n",
    "        vBER = BER(yValid, mod.predict(Xvalid))\n",
    "        tBER = BER(yTest, mod.predict(Xtest))\n",
    "\n",
    "        if vBER < bestBER:\n",
    "            bestBER = vBER\n",
    "            bestModel = mod\n",
    "            bestValidBER = vBER\n",
    "            bestTestBER = tBER\n",
    "\n",
    "    # Return the validBER and testBER for the model that works best on the validation set\n",
    "    return bestModel, bestValidBER, bestTestBER\n",
    "    # return mod, validBER, testBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da52688b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af37382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c66382ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q4(C, catID, dataTrain, dataValid, dataTest):\n",
    "    mod, validBER, testBER_noCat = pipeline(C, catID, dataTrain, dataValid, dataTest, False, True, True)\n",
    "    mod, validBER, testBER_noReview = pipeline(C, catID, dataTrain, dataValid, dataTest, True, False, True)\n",
    "    mod, validBER, testBER_noLength = pipeline(C, catID, dataTrain, dataValid, dataTest, True, True, False)\n",
    "    return testBER_noCat, testBER_noReview, testBER_noLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b763d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dbd21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "024a628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    # Jaccard similarity = |intersection| / |union|\n",
    "    intersection = s1 & s2\n",
    "    union = s1 | s2\n",
    "    return len(intersection) / len(union) if len(union) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94773001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar(i, N, usersPerItem):\n",
    "    similarities = []\n",
    "    users_i = usersPerItem[i]\n",
    "\n",
    "    for j in usersPerItem:\n",
    "        if j == i:\n",
    "            continue\n",
    "        sim = Jaccard(users_i, usersPerItem[j])\n",
    "        similarities.append((sim, j))\n",
    "\n",
    "    # Sort by similarity in descending order\n",
    "    similarities.sort(reverse=True)\n",
    "    \n",
    "    # Should be a list of (similarity, itemID) pairs\n",
    "    return similarities[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed66772f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d18b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71fc4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, ypred):\n",
    "    # Implement...\n",
    "    return sum((a - b) ** 2 for a, b in zip(y, ypred)) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf535d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanRating(dataTrain):\n",
    "    # Implement...\n",
    "    total = sum(d['star_rating'] for d in dataTrain)\n",
    "    return total / len(dataTrain)\n",
    "\n",
    "def getUserAverages(itemsPerUser, ratingDict):\n",
    "    # Implement (should return a dictionary mapping users to their averages)\n",
    "    userSum = defaultdict(float)\n",
    "    userCount = defaultdict(int)\n",
    "\n",
    "    for (user, item), rating in ratingDict.items():\n",
    "        userSum[user] += rating\n",
    "        userCount[user] += 1\n",
    "\n",
    "    userAverages = {user: userSum[user] / userCount[user] for user in userSum}\n",
    "    return userAverages\n",
    "\n",
    "def getItemAverages(usersPerItem, ratingDict):\n",
    "    # Implement...\n",
    "    itemSum = defaultdict(float)\n",
    "    itemCount = defaultdict(int)\n",
    "\n",
    "    for (user, item), rating in ratingDict.items():\n",
    "        itemSum[item] += rating\n",
    "        itemCount[item] += 1\n",
    "\n",
    "    itemAverages = {item: itemSum[item] / itemCount[item] for item in itemSum}\n",
    "    return itemAverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929248fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df9fa7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating(user,item,ratingMean,reviewsPerUser,usersPerItem,itemsPerUser,userAverages,itemAverages):\n",
    "    # Solution for Q6, should return a rating\n",
    "    if item not in itemAverages:\n",
    "        return ratingMean  # fallback to global average if item unseen\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for review in reviewsPerUser[user]:\n",
    "        j = review['product_id']\n",
    "        if j == item:\n",
    "            continue\n",
    "        users_i = usersPerItem.get(item, set())\n",
    "        users_j = usersPerItem.get(j, set())\n",
    "        sim = len(users_i & users_j) / len(users_i | users_j) if users_i | users_j else 0\n",
    "\n",
    "        if sim > 0:\n",
    "            ruj = review['star_rating']\n",
    "            rj = itemAverages.get(j, ratingMean)\n",
    "            numerator += (ruj - rj) * sim\n",
    "            denominator += sim\n",
    "\n",
    "    if denominator == 0:\n",
    "        return itemAverages[item]  # fallback to item average\n",
    "\n",
    "    return itemAverages[item] + numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608092c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7afad6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21b64e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRatingQ7(user, item, ratingMean, reviewsPerUser, usersPerItem, itemsPerUser, userAverages, itemAverages):\n",
    "    # Fallbacks\n",
    "    itemAvg = itemAverages.get(item, None)\n",
    "    userAvg = userAverages.get(user, None)\n",
    "\n",
    "    # If item is unseen, fallback to user average or global mean\n",
    "    if itemAvg is None:\n",
    "        return userAvg if userAvg is not None else ratingMean\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    # Similarity-weighted deviation from item averages\n",
    "    for review in reviewsPerUser[user]:\n",
    "        j = review['product_id']\n",
    "        if j == item:\n",
    "            continue\n",
    "        users_i = usersPerItem.get(item, set())\n",
    "        users_j = usersPerItem.get(j, set())\n",
    "        sim = len(users_i & users_j) / len(users_i | users_j) if users_i | users_j else 0\n",
    "\n",
    "        if sim > 0:\n",
    "            ruj = review['star_rating']\n",
    "            rj = itemAverages.get(j, ratingMean)\n",
    "            numerator += (ruj - rj) * sim\n",
    "            denominator += sim\n",
    "\n",
    "    # If similarity signal is strong, use it\n",
    "    if denominator > 0:\n",
    "        return itemAvg + numerator / denominator\n",
    "\n",
    "    # Otherwise, blend item and user averages if both exist\n",
    "    if userAvg is not None:\n",
    "        return 0.5 * itemAvg + 0.5 * userAvg\n",
    "\n",
    "    # Final fallback\n",
    "    return itemAvg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4384a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
