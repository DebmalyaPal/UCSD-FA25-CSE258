{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62c3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "import numpy\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74ac9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6d8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f85871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer/denom\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c51f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Rating prediction                              #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef4c9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGlobalAverage(trainRatings):\n",
    "    total = sum(trainRatings)\n",
    "    count = len(trainRatings)\n",
    "    return total / count if count > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e2d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trivialValidMSE(ratingsValid, globalAverage):\n",
    "    squared_error_sum = 0.0\n",
    "    for (_, _, actual_rating) in ratingsValid:\n",
    "        error = actual_rating - globalAverage\n",
    "        squared_error_sum += error ** 2\n",
    "    return squared_error_sum / len(ratingsValid) if ratingsValid else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bde7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    # Update equation for alpha\n",
    "    total = 0.0\n",
    "    for (u, i, r) in ratingsTrain:\n",
    "        total += r - (betaU[u] + betaI[i])\n",
    "    return total / len(ratingsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d685a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb):\n",
    "    # Update equation for betaU\n",
    "    newBetaU = {}\n",
    "    for u in ratingsPerUser:\n",
    "        total = 0.0\n",
    "        for (i, r) in ratingsPerUser[u]:\n",
    "            total += r - (alpha + betaI[i])\n",
    "        newBetaU[u] = total / (lamb + len(ratingsPerUser[u]))\n",
    "    return newBetaU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4d3af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb):\n",
    "    # Update equation for betaI\n",
    "    newBetaI = {}\n",
    "    for i in ratingsPerItem:\n",
    "        total = 0.0\n",
    "        for (u, r) in ratingsPerItem[i]:\n",
    "            total += r - (alpha + betaU[u])\n",
    "        newBetaI[i] = total / (lamb + len(ratingsPerItem[i]))\n",
    "    return newBetaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c109250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msePlusReg(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    # Compute the MSE and the mse+regularization term\n",
    "    mse = 0.0\n",
    "    for (u, i, r) in ratingsTrain:\n",
    "        pred = alpha + betaU[u] + betaI[i]\n",
    "        mse += (r - pred) ** 2\n",
    "    mse /= len(ratingsTrain)\n",
    "\n",
    "    regularizer = sum(bu**2 for bu in betaU.values()) + sum(bi**2 for bi in betaI.values())\n",
    "    mseReg = mse + lamb * regularizer\n",
    "    return mse, mseReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12d6dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validMSE(ratingsValid, alpha, betaU, betaI):\n",
    "    # Compute the MSE on the validation set\n",
    "    mse = 0\n",
    "    for (u, i, r) in ratingsValid:\n",
    "        pred = alpha + betaU.get(u, 0.0) + betaI.get(i, 0.0)\n",
    "        mse += (r - pred) ** 2\n",
    "    return mse / len(ratingsValid) if ratingsValid else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "388056f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodModel(ratingsTrain, ratingsPerUser, ratingsPerItem, alpha, betaU, betaI):\n",
    "    # Improve upon your model from the previous question (e.g. by running multiple iterations)\n",
    "    lamb = 1.0\n",
    "    maxIter = 50\n",
    "    tol = 1e-4\n",
    "    prev_mseReg = float('inf')\n",
    "    \n",
    "    for it in range(maxIter):\n",
    "        alpha = alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb)\n",
    "        betaU = betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb)\n",
    "        betaI = betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb)\n",
    "        mse, mseReg = msePlusReg(ratingsTrain, alpha, betaU, betaI, lamb)\n",
    "\n",
    "        if abs(prev_mseReg - mseReg) < tol:\n",
    "            break\n",
    "        prev_mseReg = mseReg\n",
    "\n",
    "    return alpha, betaU, betaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a670321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsRating(alpha, betaU, betaI):\n",
    "    # Write your predictions to a file that you can submit\n",
    "    predictions = open(\"predictions_Rating.csv\", 'w')\n",
    "    for l in open(\"pairs_Rating.csv\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,b = l.strip().split(',')\n",
    "        bu = 0\n",
    "        bi = 0\n",
    "        if u in betaU:\n",
    "            bu = betaU[u]\n",
    "        if b in betaI:\n",
    "            bi = betaI[b]\n",
    "        _ = predictions.write(u + ',' + b + ',' + str(alpha + bu + bi) + '\\n')\n",
    "\n",
    "    predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd75a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Read prediction                                #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04946ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateValidation(allRatings, ratingsValid):\n",
    "    # Using ratingsValid, generate two sets:\n",
    "    # readValid: set of (u,b) pairs in the validation set\n",
    "    # notRead: set of (u,b') pairs, containing one negative (not read) for each row (u) in readValid  \n",
    "    # Both should have the same size as ratingsValid\n",
    "    readValid = set()\n",
    "    notRead = set()\n",
    "\n",
    "    # Build a map of books read by each user in the training data\n",
    "    booksReadByUser = defaultdict(set)\n",
    "    allBooks = set()\n",
    "    for (u, b, r) in allRatings:\n",
    "        booksReadByUser[u].add(b)\n",
    "        allBooks.add(b)\n",
    "\n",
    "    allBooks = list(allBooks)\n",
    "    for (u, b, r) in ratingsValid:\n",
    "        readValid.add((u, b))\n",
    "        # sample one random book not read by this user\n",
    "        for _ in range(100):  # avoid infinite loop\n",
    "            neg = random.choice(allBooks)\n",
    "            if neg not in booksReadByUser[u]:\n",
    "                notRead.add((u, neg))\n",
    "                break\n",
    "\n",
    "    return readValid, notRead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afdf7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseLineStrategy(mostPopular, totalRead):\n",
    "    return1 = set()\n",
    "\n",
    "    # Compute the set of items for which we should return \"True\"\n",
    "    # This is the same strategy implemented in the baseline code for Assignment 1\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalRead/2: break\n",
    "\n",
    "    return return1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8c7c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improvedStrategy(mostPopular, totalRead):\n",
    "    return1 = set()\n",
    "\n",
    "    # Same as above function, just find an item set that'll have higher accuracy\n",
    "\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        # predict True for top ~70% of reads instead of 50%\n",
    "        if count > totalRead * 0.7:\n",
    "            break\n",
    "\n",
    "    return return1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "879652f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateStrategy(return1, readValid, notRead):\n",
    "\n",
    "    # Compute the accuracy of a strategy which just returns \"true\" for a set of items (those in return1)\n",
    "    # readValid: instances with positive label\n",
    "    # notRead: instances with negative label\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x in readValid:\n",
    "        total += 1\n",
    "        if x[1] in return1:\n",
    "            correct += 1\n",
    "\n",
    "    for x in notRead:\n",
    "        total += 1\n",
    "        if x[1] not in return1:\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1acbf0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardThresh(u,b,ratingsPerItem,ratingsPerUser):\n",
    "    \n",
    "    # Compute the similarity of the query item (b) compared to the most similar item in the user's history\n",
    "    # Return true if the similarity is high or the item is popular\n",
    "    \n",
    "    maxSim = 0\n",
    "    userItems = ratingsPerUser.get(u, [])\n",
    "    itemUsers = ratingsPerItem.get(b, [])\n",
    "\n",
    "    users_b = set([x for x in itemUsers])\n",
    "    for b2 in userItems:\n",
    "        users_b2 = set([x for x in ratingsPerItem.get(b2, [])])\n",
    "        sim = Jaccard(users_b, users_b2)\n",
    "        if sim > maxSim:\n",
    "            maxSim = sim\n",
    "    \n",
    "    if maxSim > 0.013 or len(ratingsPerItem[b]) > 40: # Keep these thresholds as-is\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13664877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsRead(ratingsPerItem, ratingsPerUser):\n",
    "    predictions = open(\"predictions_Read.csv\", 'w')\n",
    "    for l in open(\"pairs_Read.csv\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,b = l.strip().split(',')\n",
    "        pred = jaccardThresh(u,b,ratingsPerItem,ratingsPerUser)\n",
    "        _ = predictions.write(u + ',' + b + ',' + str(pred) + '\\n')\n",
    "\n",
    "    predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c1e31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Category prediction                            #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "445775f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureCat(datum, words, wordId, wordSet):\n",
    "    feat = [0]*len(words)\n",
    "\n",
    "    # Compute features counting instance of each word in \"words\"\n",
    "    # after converting to lower case and removing punctuation\n",
    "    \n",
    "    review_text = datum['review_text'].lower()\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    review_text = review_text.translate(translator)\n",
    "    for w in review_text.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    \n",
    "    feat.append(1) # offset (put at the end)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4985733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betterFeatures(data):\n",
    "    \n",
    "    # Produce better features than those from the above question\n",
    "    # Return matrix (each row is the feature vector for one entry in the dataset)\n",
    "\n",
    "    wordCount = defaultdict(int)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    for d in data:\n",
    "        review_text = d['review_text'].lower()\n",
    "        review_text = review_text.translate(translator)\n",
    "        for w in review_text.split():\n",
    "            wordCount[w] += 1\n",
    "\n",
    "    # Use top 1000 words instead of 500\n",
    "    topWords = [x[0] for x in sorted(wordCount.items(), key=lambda x: -x[1])[:1000]]\n",
    "    wordId = {w: i for i, w in enumerate(topWords)}\n",
    "    wordSet = set(topWords)\n",
    "\n",
    "    X = []\n",
    "    for d in data:\n",
    "        feat = [0]*len(topWords)\n",
    "        review_text = d['review_text'].lower().translate(translator)\n",
    "        for w in review_text.split():\n",
    "            if w in wordSet:\n",
    "                feat[wordId[w]] += 1\n",
    "        feat.append(1)\n",
    "        X.append(feat)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36e24732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runOnTest(data_test, mod):\n",
    "    Xtest = [featureCat(d) for d in data_test]\n",
    "    pred_test = mod.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72359012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsCategory(pred_test):\n",
    "    predictions = open(\"predictions_Category.csv\", 'w')\n",
    "    pos = 0\n",
    "\n",
    "    for l in open(\"pairs_Category.csv\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,b = l.strip().split(',')\n",
    "        _ = predictions.write(u + ',' + b + ',' + str(pred_test[pos]) + '\\n')\n",
    "        pos += 1\n",
    "\n",
    "    predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bd24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
